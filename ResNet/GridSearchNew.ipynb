{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyNIKRY3HFh8Sp/26hxXvqjD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"csZ46OZHjiZI","outputId":"e5541a22-1725-44e9-b347-56ef19281e3f","executionInfo":{"status":"ok","timestamp":1735364217612,"user_tz":-120,"elapsed":591996,"user":{"displayName":"Konstantinos Roumeliotis","userId":"17264923090131634662"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","[{'learning_rate': 0.0001, 'batch_size': 32, 'image_size': 200, 'num_epochs': 3}]\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 27/27 [02:33<00:00,  5.67s/it]\n","Training: 100%|██████████| 27/27 [02:31<00:00,  5.63s/it]\n","Training: 100%|██████████| 27/27 [02:31<00:00,  5.62s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Configuration: {'learning_rate': 0.0001, 'batch_size': 32, 'image_size': 200, 'num_epochs': 3}\n","Training Results: {'timestamp': '2024-12-28T05:36:56.900840', 'configuration': {'learning_rate': 0.0001, 'batch_size': 32, 'image_size': 200, 'num_epochs': 3}, 'performance': {'final_accuracy': 44.44444444444444, 'final_loss': 1.8288670778274536, 'precision': 0.4747777777777778, 'recall': 0.4444444444444444, 'f1_score': 0.43662403605951994, 'training_time': 194.923321723938}}\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import pandas as pd\n","import requests\n","from io import BytesIO\n","import time\n","from tqdm import tqdm\n","import os\n","import logging\n","import gc\n","from itertools import product\n","import json\n","from datetime import datetime\n","import numpy as np\n","from sklearn.metrics import precision_recall_fscore_support\n","import multiprocessing\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# Setup logging\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    handlers=[\n","        logging.FileHandler('training.log'),\n","        logging.StreamHandler()\n","    ]\n",")\n","logger = logging.getLogger(__name__)\n","\n","def save_best_model(model, label_to_idx, config, val_metrics, epoch, best_val_loss, best_model_filename):\n","    if val_metrics['loss'] < best_val_loss:\n","        best_val_loss = val_metrics['loss']\n","        config_filename = f\"model_lr_{config['hyperparameters']['learning_rate']}_bs_{config['hyperparameters']['batch_size']}_es_{config['hyperparameters']['image_size']}.pth\"\n","        model_path = os.path.join(config['paths']['absolute_path'], config_filename)\n","\n","        torch.save(model.state_dict(), model_path)\n","\n","        label_to_idx_filename = model_path.replace('.pth', '_label_to_idx.json')\n","        with open(label_to_idx_filename, 'w') as f:\n","            json.dump(label_to_idx, f)\n","\n","        logger.info(f\"Best model saved at epoch {epoch} with loss {val_metrics['loss']}\")\n","\n","    return best_val_loss\n","\n","class Config:\n","    @staticmethod\n","    def validate_config(config):\n","        \"\"\"\n","        Validates the provided configuration to ensure all necessary paths, files, and parameters are correctly defined.\n","        - Checks for required paths and files in the dataset.\n","        - Ensures that hyperparameters like batch size, learning rate, and image size are positive.\n","        \"\"\"\n","\n","        required_paths = ['absolute_path', 'dataset_path']\n","        required_files = ['train_file', 'validation_file']\n","        required_columns = ['feature_col', 'label_col']\n","\n","        # Validate paths\n","        for path in required_paths:\n","            if not os.path.exists(config['paths'][path]):\n","                raise ValueError(f\"Path not found: {config['paths'][path]}\")\n","\n","        # Validate files\n","        for file in required_files:\n","            file_path = os.path.join(config['paths']['dataset_path'], config['filenames'][file])\n","            if not os.path.exists(file_path):\n","                raise ValueError(f\"File not found: {file_path}\")\n","\n","        # Validate hyperparameters\n","        if config['hyperparameters']['batch_size'] <= 0:\n","            raise ValueError(\"Batch size must be positive\")\n","        if config['hyperparameters']['learning_rate'] <= 0:\n","            raise ValueError(\"Learning rate must be positive\")\n","        if config['hyperparameters']['image_size'] <= 0:\n","            raise ValueError(\"Image size must be positive\")\n","\n","        return True\n","\n","class CustomImageDataset(Dataset):\n","    \"\"\"\n","    A custom PyTorch Dataset for loading images from URLs, with optional caching of images locally.\n","    \"\"\"\n","\n","    # Initializes the dataset, reads the CSV file, and prepares label mappings.\n","    def __init__(self, csv_file, config, transform=None, cache_dir=None):\n","        self.data = pd.read_csv(os.path.join(config['paths']['dataset_path'], csv_file))\n","        self.transform = transform\n","        self.feature_col = config['columns']['feature_col']\n","        self.label_col = config['columns']['label_col']\n","        # Creates a sorted list of unique classes (labels) from the dataset.\n","        self.classes = sorted(self.data[self.label_col].unique())\n","        # Maps each class label to an index for numerical representation.\n","        self.label_to_idx = {label: idx for idx, label in enumerate(self.classes)}\n","        # Sets the directory for caching images locally.\n","        self.cache_dir = cache_dir\n","\n","        if cache_dir and not os.path.exists(cache_dir):\n","            os.makedirs(cache_dir)\n","\n","    # Returns the number of samples in the dataset.\n","    def __len__(self):\n","        return len(self.data)\n","\n","    # Retrieves the image from the cache or downloads it from a URL.\n","    def _load_image_from_cache(self, url, idx):\n","        if self.cache_dir:\n","            cache_path = os.path.join(self.cache_dir, f\"img_{idx}.jpg\")\n","            if os.path.exists(cache_path):\n","                return Image.open(cache_path).convert('RGB')\n","\n","        response = requests.get(url, timeout=10)\n","        if response.status_code != 200:\n","            raise ValueError(f\"Failed to fetch image: HTTP {response.status_code}\")\n","\n","        img = Image.open(BytesIO(response.content)).convert('RGB')\n","\n","        if self.cache_dir:\n","            img.save(cache_path)\n","\n","        return img\n","\n","    # Loads an image and its corresponding label\n","    def __getitem__(self, idx):\n","        try:\n","            img_url = self.data.iloc[idx][self.feature_col]\n","            label = self.data.iloc[idx][self.label_col]\n","\n","            img = self._load_image_from_cache(img_url, idx)\n","\n","            if self.transform:\n","                img = self.transform(img)\n","\n","            label_idx = self.label_to_idx[label]\n","            return img, label_idx\n","\n","        except Exception as e:\n","            logger.error(f\"Error loading image at index {idx}: {str(e)}\")\n","            raise\n","\n","def clear_gpu_memory():\n","    \"\"\"\n","    Clears GPU memory to avoid memory overflow issues during training.\n","    - Uses PyTorch's built-in functions to release GPU memory.\n","    \"\"\"\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","class ModelTrainer:\n","    def __init__(self, config):\n","        self.config = config\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.metrics_history = []\n","        self.label_to_idx = None\n","\n","    # Prepares data augmentations and preprocessing steps for training and validation datasets.\n","    def _create_transforms(self):\n","        # Define transformations for the training dataset.\n","        # For our small dataset, more augmentations could help avoiding overfitting.\n","        # Each time an image is passed through the data loader,\n","        # these transformations are applied with randomized parameters.\n","        train_transform = transforms.Compose([\n","            # Resizes the image to a square defined by the configured image size.\n","            transforms.Resize((self.config['hyperparameters']['image_size'],\n","                           self.config['hyperparameters']['image_size'])),\n","            # Randomly flips the image horizontally.\n","            transforms.RandomHorizontalFlip(),\n","            # Randomly rotates the image by up to 15 degrees.\n","            transforms.RandomRotation(15),\n","            # Randomly changes the brightness, contrast, and saturation of the image.\n","            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n","            transforms.ToTensor(), # Converts the image into a PyTorch tensor.\n","            # Normalizes the image using the specified mean\n","            # and standard deviation values (pre-trained model standards).\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225])\n","        ])\n","\n","        # Define transformations for the validation dataset (no data augmentation).\n","        val_transform = transforms.Compose([\n","            transforms.Resize((self.config['hyperparameters']['image_size'],\n","                           self.config['hyperparameters']['image_size'])),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225])\n","        ])\n","\n","        return train_transform, val_transform\n","\n","    # Creates PyTorch DataLoader objects for efficient data loading during training and validation.\n","    def _create_dataloaders(self, train_transform, val_transform):\n","        train_dataset = CustomImageDataset(\n","            self.config['filenames']['train_file'],\n","            self.config,\n","            transform=train_transform\n","        )\n","\n","        self.label_to_idx = train_dataset.label_to_idx  # Store the mapping\n","\n","        validation_dataset = CustomImageDataset(\n","            self.config['filenames']['validation_file'],\n","            self.config,\n","            transform=val_transform\n","        )\n","\n","        num_workers = min(multiprocessing.cpu_count(), 4)\n","\n","        train_loader = DataLoader(\n","            train_dataset,\n","            batch_size=self.config['hyperparameters']['batch_size'],\n","            shuffle=True,\n","            num_workers=num_workers,\n","            pin_memory=True\n","        )\n","\n","        validation_loader = DataLoader(\n","            validation_dataset,\n","            batch_size=self.config['hyperparameters']['batch_size'],\n","            shuffle=False,\n","            num_workers=num_workers,\n","            pin_memory=True\n","        )\n","\n","        return train_loader, validation_loader, train_dataset.classes\n","\n","    # Configures a ResNet50 model with fine-tuning of specific layers for the given number of classes.\n","    def _create_model(self, num_classes):\n","        # Loads a pre-trained ResNet-50 model with weights trained on the ImageNet dataset.\n","        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n","        # Replaces the fully connected (fc) layer with a new sequential layer consisting of:\n","        # 1. Dropout layer (with a probability of 0.5) to reduce overfitting.\n","        # 2. Linear layer to adjust the output to match the number of classes in the dataset.\n","        model.fc = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(model.fc.in_features, num_classes)\n","        )\n","        # Moves the model to the specified device (GPU) for training.\n","        model = model.to(self.device)\n","\n","        # Freezes the early layers of the model to prevent\n","        # their weights from being updated during training.\n","        for param in model.parameters():\n","            param.requires_grad = False\n","\n","        # Unfreezes the parameters of the final convolutional block (layer4)\n","        # to allow fine-tuning.\n","        for param in model.layer4.parameters():\n","            param.requires_grad = True\n","        for param in model.fc.parameters():\n","            param.requires_grad = True\n","\n","        return model\n","\n","    # Performs one epoch of training and computes training metrics like loss and accuracy.\n","    def _train_epoch(self, model, train_loader, criterion, optimizer):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","        all_preds = [] # List to store all predicted labels for the epoch.\n","        all_labels = [] # List to store all true labels for the epoch.\n","\n","        # Loops through the training data loader batch by batch.\n","        for inputs, labels in tqdm(train_loader, desc='Training'):\n","            # Moves the inputs and labels to the configured device (GPU).\n","            inputs, labels = inputs.to(self.device), labels.to(self.device)\n","\n","            # Clears the gradients of the optimizer to prepare for the current batch.\n","            optimizer.zero_grad()\n","            # Passes the inputs through the model to obtain outputs (predictions).\n","            outputs = model(inputs)\n","            # Computes the loss between the predictions and the true labels.\n","            loss = criterion(outputs, labels)\n","            # Backpropagates the loss to compute gradients for all trainable parameters.\n","            loss.backward()\n","\n","            # Clips gradients to avoid exploding gradients during backpropagation.\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            optimizer.step() # Updates the model parameters using the optimizer.\n","\n","            # Accumulates the loss for reporting the average loss over the epoch.\n","            running_loss += loss.item()\n","            # Gets the predicted class labels for the batch by taking the index\n","            # of the maximum value in each output vector.\n","            _, predicted = torch.max(outputs.data, 1)\n","            # Updates the total number of labels and the count of correct\n","            # predictions for accuracy calculation.\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","            # Appends the predicted and true labels for this batch\n","            # to the lists for further analysis.\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","        return {\n","            'loss': running_loss / len(train_loader),\n","            'accuracy': 100 * correct / total,\n","            'predictions': np.array(all_preds),\n","            'labels': np.array(all_labels)\n","        }\n","\n","    # Evaluates the model on the validation dataset and computes metrics like precision, recall, and F1-score.\n","    def _validate(self, model, validation_loader, criterion):\n","        model.eval()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","        all_preds = []\n","        all_labels = []\n","\n","        with torch.no_grad():\n","            for inputs, labels in validation_loader:\n","                inputs, labels = inputs.to(self.device), labels.to(self.device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","\n","                running_loss += loss.item()\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","                all_preds.extend(predicted.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","        return {\n","            'loss': running_loss / len(validation_loader),\n","            'accuracy': 100 * correct / total,\n","            'predictions': np.array(all_preds),\n","            'labels': np.array(all_labels)\n","        }\n","\n","    def train(self, patience=4):\n","        try:\n","            train_transform, val_transform = self._create_transforms()\n","            train_loader, validation_loader, classes = self._create_dataloaders(\n","                train_transform, val_transform)\n","\n","            model = self._create_model(len(classes))\n","            criterion = nn.CrossEntropyLoss()\n","            optimizer = torch.optim.Adam([\n","                {'params': model.fc.parameters(),\n","                'lr': self.config['hyperparameters']['learning_rate'] * 10},\n","                {'params': model.layer4.parameters(),\n","                'lr': self.config['hyperparameters']['learning_rate']}\n","            ])\n","            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","                optimizer, mode='max', patience=1, factor=0.1)\n","\n","            best_val_loss = float('inf')\n","            early_stop_counter = 0\n","            epoch_metrics = []\n","\n","            for epoch in range(self.config['hyperparameters']['num_epochs']):\n","                epoch_start = time.time()\n","\n","                train_metrics = self._train_epoch(model, train_loader, criterion, optimizer)\n","                clear_gpu_memory()\n","                val_metrics = self._validate(model, validation_loader, criterion)\n","\n","                precision, recall, f1, _ = precision_recall_fscore_support(\n","                    val_metrics['labels'],\n","                    val_metrics['predictions'],\n","                    average='weighted',\n","                    zero_division=0\n","                )\n","\n","                val_metrics.update({\n","                    'precision': precision,\n","                    'recall': recall,\n","                    'f1': f1\n","                })\n","\n","                epoch_time = time.time() - epoch_start\n","\n","                epoch_metrics.append({\n","                    'epoch': epoch + 1,\n","                    'train': train_metrics,\n","                    'validation': val_metrics,\n","                    'time': epoch_time\n","                })\n","\n","                scheduler.step(val_metrics['accuracy'])\n","\n","                best_val_loss = save_best_model(\n","                    model, self.label_to_idx, self.config, val_metrics,\n","                    epoch + 1, best_val_loss, \"best_model.pth\"\n","                )\n","\n","                if val_metrics['loss'] < best_val_loss:\n","                    best_val_loss = val_metrics['loss']\n","                    early_stop_counter = 0\n","                else:\n","                    early_stop_counter += 1\n","\n","                if early_stop_counter >= patience:\n","                    logger.info(\"Early stopping triggered.\")\n","                    break\n","\n","            return epoch_metrics\n","\n","        except Exception as e:\n","            logger.error(f\"Training failed: {str(e)}\")\n","            raise\n","\n","\n","    # Saves training results and configurations for reproducibility and analysis.\n","    def track_training_results(self, config, metrics):\n","        final_epoch_metrics = metrics[-1]\n","        return {\n","            'timestamp': datetime.now().isoformat(),\n","            'configuration': {\n","                'learning_rate': config['hyperparameters']['learning_rate'],\n","                'batch_size': config['hyperparameters']['batch_size'],\n","                'image_size': config['hyperparameters']['image_size'],\n","                'num_epochs': config['hyperparameters']['num_epochs']\n","            },\n","            'performance': {\n","                'final_accuracy': final_epoch_metrics['validation']['accuracy'],\n","                'final_loss': final_epoch_metrics['validation']['loss'],\n","                'precision': final_epoch_metrics['validation']['precision'],\n","                'recall': final_epoch_metrics['validation']['recall'],\n","                'f1_score': final_epoch_metrics['validation']['f1'],\n","                'training_time': final_epoch_metrics['time']\n","            }\n","        }\n","\n","def load_previous_results(results_file):\n","    \"\"\"\n","    Reads and parses the JSON file storing previous training results to avoid redundant training.\n","    - Returns a list of past results and tested parameter configurations.\n","    \"\"\"\n","    try:\n","        with open(results_file, 'r') as f:\n","            all_results = json.load(f)\n","            # Extract the configurations that have been already tested\n","            tried_params = [result['configuration'] for result in all_results]\n","            return all_results, tried_params\n","    except (FileNotFoundError, json.JSONDecodeError):\n","        # If the file doesn't exist or is empty, return empty list\n","        return [], []\n","\n","def grid_search(param_grid, base_config):\n","    \"\"\"\n","    Performs hyperparameter tuning using a grid search over the parameter space.\n","    - Loads previously tested configurations to skip redundant experiments.\n","    - For each parameter combination:\n","        - Updates the configuration.\n","        - Trains the model and tracks metrics.\n","        - Saves results in a JSON file for reproducibility.\n","    - Returns all results from the search.\n","    \"\"\"\n","    results_file = base_config['paths']['absolute_path'] + 'training_results.json'\n","\n","    # Load previous results to avoid re-testing the same combinations\n","    all_results, tried_params = load_previous_results(results_file)\n","\n","    # Generate all combinations of hyperparameters\n","    param_combinations = [dict(zip(param_grid.keys(), v)) for v in product(*param_grid.values())]\n","\n","    # Filter out already tried combinations\n","    remaining_combinations = [params for params in param_combinations if params not in tried_params]\n","\n","    print(remaining_combinations)\n","\n","    # If no combinations are left to test\n","    if not remaining_combinations:\n","        print(\"All parameter combinations have already been tested.\")\n","        return all_results\n","\n","    # Proceed with the remaining combinations\n","    for params in remaining_combinations:\n","        current_config = base_config.copy()\n","        current_config['hyperparameters'].update(params)\n","\n","        trainer = ModelTrainer(current_config)\n","        metrics = trainer.train()\n","\n","        results = trainer.track_training_results(current_config, metrics)\n","\n","        print(f\"Configuration: {params}\")\n","        print(f\"Training Results: {results}\")\n","\n","        # Append the result to the list\n","        all_results.append(results)\n","\n","        # Write the updated results list back to the file\n","        with open(results_file, 'w') as f:\n","            json.dump(all_results, f, indent=2)\n","\n","    return all_results\n","\n","def main():\n","    \"\"\"\n","    The main entry point for running the training pipeline.\n","    - Sets up the base configuration and parameter grid.\n","    - Creates necessary directories.\n","    - Executes a grid search for hyperparameter tuning.\n","    - Logs results upon completion.\n","    \"\"\"\n","    # Base configuration\n","    base_config = {\n","        'paths': {\n","            'absolute_path': \"/content/gdrive/My Drive/Projects/ResNet/\",\n","            'dataset_path': \"/content/gdrive/My Drive/Projects/ResNet/Datasets/\"\n","        },\n","        'filenames': {\n","            'train_file': 'train_set_400.csv',\n","            'validation_file': 'validation_set_400.csv'\n","        },\n","        'columns': {\n","            'feature_col': 'Image',\n","            'label_col': 'Category'\n","        },\n","        'hyperparameters': {  # this is just a template\n","            'learning_rate': 1e-4,\n","            'num_epochs': 10,\n","            'batch_size': 32,\n","            'image_size': 224\n","        }\n","    }\n","\n","    # Parameter grid for search\n","    param_grid = {\n","        'learning_rate': [1e-3, 5e-4, 1e-4],\n","        'batch_size': [1, 16, 32, 64, 128, 256],\n","        'image_size': [100, 200, 400],\n","        'num_epochs': [3, 6, 12, 24, 30]\n","    }\n","\n","    try:\n","        # Create necessary directories\n","        os.makedirs(base_config['paths']['absolute_path'], exist_ok=True)\n","        os.makedirs(base_config['paths']['dataset_path'], exist_ok=True)\n","\n","        # Run grid search\n","        results = grid_search(param_grid, base_config)\n","\n","        logger.info(\"\\nGrid Search Completed! Check the training_results.json\")\n","\n","    except Exception as e:\n","        logger.error(f\"Error in main: {str(e)}\")\n","        raise\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"markdown","source":["# Predictions Phase"],"metadata":{"id":"kYde21X-9Dzt"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import pandas as pd\n","import requests\n","from io import BytesIO\n","import json\n","import logging\n","import os\n","import time\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# Setup logging\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    handlers=[\n","        logging.FileHandler('prediction.log'),\n","        logging.StreamHandler()\n","    ]\n",")\n","logger = logging.getLogger(__name__)\n","\n","class TestImageDataset(Dataset):\n","    \"\"\"Custom Dataset for loading test images\"\"\"\n","    def __init__(self, csv_file, feature_col, transform=None):\n","        self.data = pd.read_csv(csv_file)\n","        self.feature_col = feature_col\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        try:\n","            img_url = self.data.iloc[idx][self.feature_col]\n","\n","            # Download and open image\n","            response = requests.get(img_url, timeout=10)\n","            if response.status_code != 200:\n","                raise ValueError(f\"Failed to fetch image: HTTP {response.status_code}\")\n","\n","            img = Image.open(BytesIO(response.content)).convert('RGB')\n","\n","            if self.transform:\n","                img = self.transform(img)\n","\n","            return img, idx\n","\n","        except Exception as e:\n","            logger.error(f\"Error loading image at index {idx}: {str(e)}\")\n","            raise\n","\n","def load_model_and_labels(model_path, label_to_idx_path):\n","    \"\"\"Load the trained model and label mapping\"\"\"\n","    try:\n","        # Load label mapping\n","        with open(label_to_idx_path, 'r') as f:\n","            label_to_idx = json.load(f)\n","\n","        # Create inverse mapping\n","        idx_to_label = {v: k for k, v in label_to_idx.items()}\n","\n","        # Initialize model\n","        model = models.resnet50(weights=None)\n","        model.fc = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(model.fc.in_features, len(label_to_idx))\n","        )\n","\n","        # Load trained weights\n","        model.load_state_dict(torch.load(model_path))\n","\n","        return model, idx_to_label\n","\n","    except Exception as e:\n","        logger.error(f\"Error loading model and labels: {str(e)}\")\n","        raise\n","\n","def predict_images(test_set_path, model_path, label_to_idx_path, batch_size,\n","                  prediction_col_name, output_path, feature_col='Image'):\n","    \"\"\"\n","    Make predictions on test images and save results\n","\n","    Parameters:\n","    - test_set_path: path to test CSV file\n","    - model_path: path to trained model weights\n","    - label_to_idx_path: path to label mapping JSON\n","    - batch_size: batch size for predictions\n","    - prediction_col_name: name for the new predictions column\n","    - output_path: path to save predictions CSV\n","    - feature_col: name of column containing image URLs\n","\n","    Returns:\n","    - result_df: DataFrame with predictions\n","    - execution_time: Time taken for predictions in seconds\n","    - prediction_cost: Cost of predictions based on execution time\n","    \"\"\"\n","    try:\n","        # Start timing\n","        start_time = time.time()\n","\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        logger.info(f\"Using device: {device}\")\n","\n","        # Load test data\n","        test_df = pd.read_csv(test_set_path)\n","        logger.info(f\"Loaded test set with {len(test_df)} images\")\n","\n","        # Create transforms for test images\n","        test_transform = transforms.Compose([\n","            transforms.Resize((224, 224)),  # Standard ResNet input size\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                              std=[0.229, 0.224, 0.225])\n","        ])\n","\n","        # Create dataset and dataloader\n","        test_dataset = TestImageDataset(test_set_path, feature_col, test_transform)\n","        test_loader = DataLoader(test_dataset,\n","                               batch_size=batch_size,\n","                               shuffle=False,\n","                               num_workers=4)\n","\n","        # Load model and label mapping\n","        model, idx_to_label = load_model_and_labels(model_path, label_to_idx_path)\n","        model = model.to(device)\n","        model.eval()\n","\n","        # Make predictions\n","        predictions = []\n","        with torch.no_grad():\n","            for batch_images, batch_indices in test_loader:\n","                batch_images = batch_images.to(device)\n","                outputs = model(batch_images)\n","                _, predicted = torch.max(outputs.data, 1)\n","\n","                # Convert indices to labels\n","                batch_predictions = [idx_to_label[idx.item()]\n","                                  for idx in predicted]\n","\n","                # Store predictions with their indices\n","                for idx, pred in zip(batch_indices, batch_predictions):\n","                    predictions.append((idx.item(), pred))\n","\n","        # Sort predictions by index to maintain original order\n","        predictions.sort(key=lambda x: x[0])\n","        predicted_labels = [pred[1] for pred in predictions]\n","\n","        # Add predictions to dataframe\n","        test_df[prediction_col_name] = predicted_labels\n","\n","        # Save results\n","        test_df.to_csv(output_path, index=False)\n","\n","        # Calculate execution time and cost\n","        execution_time = time.time() - start_time\n","        prediction_cost = 0.000281392488 * execution_time\n","\n","        logger.info(f\"Predictions saved to {output_path}\")\n","        logger.info(f\"Prediction time: {execution_time:.2f} seconds\")\n","        logger.info(f\"Prediction cost: ${prediction_cost:.6f}\")\n","\n","        return test_df, execution_time, prediction_cost\n","\n","    except Exception as e:\n","        logger.error(f\"Error in prediction pipeline: {str(e)}\")\n","        raise\n","\n","absolute_path = \"/content/gdrive/My Drive/Projects/ResNet/\"\n","\n","if __name__ == \"__main__\":\n","    test_params = {\n","        'test_set_path': absolute_path + 'Datasets/test_set_100.csv',\n","        'model_path': absolute_path + 'model_lr_0.0001_bs_32_es_100.pth',\n","        'label_to_idx_path': absolute_path + 'model_lr_0.0001_bs_32_es_100_label_to_idx.json',\n","        'batch_size': 32,\n","        'prediction_col_name': 'ResNet50-Predictions',\n","        'output_path': absolute_path + 'Datasets/test_set_100_with_predictions.csv'\n","    }\n","\n","    # Run predictions\n","    result_df, execution_time, prediction_cost = predict_images(**test_params)\n","\n","    print(\"\\nPrediction Results Summary:\")\n","    print(f\"Total prediction time: {execution_time:.2f} seconds\")\n","    print(f\"Total prediction cost: ${prediction_cost:.6f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3zD2tixDDEM9","executionInfo":{"status":"ok","timestamp":1735369178492,"user_tz":-120,"elapsed":27620,"user":{"displayName":"Konstantinos Roumeliotis","userId":"17264923090131634662"}},"outputId":"97746190-a72c-4416-91c6-02593bdaaf84"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-8-44ce3043eb9f>:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_path))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Prediction Results Summary:\n","Total prediction time: 25.74 seconds\n","Total prediction cost: $0.007242\n"]}]}]}